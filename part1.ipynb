{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning with PyTorch** -  Introductory Lab\n",
    "\n",
    "## **Part 1:** PyTorch Basics\n",
    "\n",
    "* Here we will look at [**PyTorch**](https://pytorch.org/)\n",
    "* It is the most popular deep learning library, together with Google's TensorFlow\n",
    "* You can easily search in the [full documentation](https://pytorch.org/docs/stable/index.html)\n",
    "* There is also a good [starting tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* We will also cover the basics here.\n",
    "\n",
    "### **Important:**\n",
    "* Go through, **run** and **understand** the code in each cell\n",
    "* It is important that you also **modify**, **explore**, and **play around** with the code :)\n",
    "* There are also some **exercises** marked as:\n",
    "### ðŸ’¡ **Exercise**\n",
    "* Now, lets start ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pytorch library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D tensor with random values\n",
    "\n",
    "x = torch.rand(3,4)    # The arguments indicate the shape\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the shape of the tensor\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be any number of dimensions\n",
    "x1 = torch.rand(4,2,6,3,8)\n",
    "\n",
    "# Tensor with standard Normal distributed values\n",
    "x2 = torch.randn(2,3)\n",
    "print(x2)\n",
    "\n",
    "# Tensor with zeros\n",
    "x3 = torch.zeros(2,3)\n",
    "print(x3)\n",
    "\n",
    "# Tensor with ones\n",
    "x4 = torch.ones(2,3)\n",
    "print(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor with increasing values\n",
    "x5 = torch.arange(6)\n",
    "print(x5)\n",
    "\n",
    "# Create a tensor with your favourite values\n",
    "my_tensor = torch.Tensor([1.41, 5.1, 3.1415])\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Tensors\n",
    "* Tensors can have different data types\n",
    "* The type can be checked through the `dtype` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3)\n",
    "print(x.dtype)   # Print the type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This was a `float32` tensor. That is the standard type.\n",
    "* There are other useful types ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a double precision tensor\n",
    "x = torch.rand(3, dtype=torch.float64)\n",
    "print(x.dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `float64` tensors are useful if you need extra numerical precision\n",
    "* But this is seldom needed for deep learning\n",
    "* `float32` consumes half the memory and are therefore preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with integers\n",
    "x = torch.LongTensor([3, 6, -11])\n",
    "print(x.dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dividing the elements with 2\n",
    "x / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That only works for float tensors. Use integer division instead\n",
    "x // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images often comes as unsigned 8-bit tensors (or Byte tensors)\n",
    "x = torch.ByteTensor([3, 0, 100, 255])\n",
    "print(x)\n",
    "print(x + 1)  # Note that 255 + 1 = 0  (i.e. modulo 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert from one tensor to another\n",
    "x_float = 10 * torch.rand(3)\n",
    "x_long = x_float.long()\n",
    "x_float2 = x_long.float()\n",
    "\n",
    "print('x_float =', x_float)\n",
    "print('x_long =', x_long)\n",
    "print('x_float2 =', x_float2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating on Tensors\n",
    "\n",
    "#### Point-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "y = torch.rand(3,4)\n",
    "\n",
    "# You can play around with what you can do.\n",
    "# These operations are pointwise\n",
    "# Add print statements if you want to see the outputs\n",
    "z = x + y\n",
    "z = x / y\n",
    "z = x**y\n",
    "z = torch.exp(x)\n",
    "z = torch.sin(x)\n",
    "z = x.exp()\n",
    "z = x.round()\n",
    "z = (x - 0.5).abs()\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "# Sum all elements\n",
    "print(torch.sum(x))\n",
    "print(x.sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# Sum along a dimension\n",
    "print(x.sum(dim=0))\n",
    "print(x.sum(dim=1))\n",
    "\n",
    "print()\n",
    "\n",
    "# Keep dimensions after summing\n",
    "print(x.shape)\n",
    "print(x.sum(dim=0).shape)\n",
    "print(x.sum(dim=0, keepdim=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "# You can calso do a mean\n",
    "print('Mean: ', x.mean())\n",
    "\n",
    "# Max and min works in a similar way\n",
    "print('Min: ', x.min())\n",
    "print('Max: ', x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use .item() to convert a scalar tensor to just a number\n",
    "s = x.sum()\n",
    "print(s)\n",
    "print(s.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With max/min along a dimension you also get the argmax/argmin\n",
    "max_val, arg_max = x.max(dim=0)\n",
    "print(max_val)\n",
    "print(arg_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ **Exercise**\n",
    "* Using the functions above, write a function `argmin2d` in the cell below that returns the 2d-coordinate of the minimum value of any input 2d tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmin2d(x):\n",
    "    pass   # Write your code here\n",
    "\n",
    "x = torch.rand(5,6)\n",
    "\n",
    "print(x)\n",
    "print(argmin2d(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inplace operations\n",
    "* These operations do not create a new tensor\n",
    "* Instead they modify the values inside the tensor itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3)\n",
    "print(x)\n",
    "\n",
    "x += torch.ones(3)\n",
    "print(x)\n",
    "\n",
    "x.exp_()    # PyTorch functions ending with _ are inplace\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "* Broadcasting is an important concept when performing an operation that involves two tensors.\n",
    "* If the tensors have different shapes, then some dimensions are automatically expanded (replicated) without copying the underlying data.\n",
    "* This is both very convenient and useful.\n",
    "* It is easies to learn from some examples ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the tensors have an equal number of dimensions\n",
    "# The dimension of size 1 is always expanded to match that of the other tensor\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "y = torch.rand(1,4)\n",
    "z = torch.rand(3,1)\n",
    "\n",
    "print(x + y)   # First dimension in y is broadcasted\n",
    "print(x * z)   # Second dimension in z is broadcasted\n",
    "print(y / z)   # Both the first dimension in y and the second dimension in z are broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This still gives an error since dimension 0 has non-matching sizes\n",
    "# So, each dimension in y must either be 1 or the same size as in x\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "y = torch.rand(2,4)\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting also works if the number of dimensions are different in the two tensors.\n",
    "# But be careful when using it. The behavior is not that intuitive unless you are used to it!\n",
    "# Lets check some examples ...\n",
    "\n",
    "x = torch.rand(3,6,4)\n",
    "y1 = torch.rand(4)\n",
    "y2 = torch.rand(6,4)\n",
    "y3 = torch.rand(6,1)\n",
    "\n",
    "# All these examples work!\n",
    "# Note that the dimensions are aligned by starting from the last one.\n",
    "# Therefore, the first dimension in x is always broadcasted in this example\n",
    "\n",
    "z = x + y1\n",
    "z = x + y2\n",
    "z = x + y3\n",
    "\n",
    "print('No error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot align the tensors starting from the first dimension\n",
    "\n",
    "x = torch.rand(3,6,4)\n",
    "y = torch.rand(3,6)\n",
    "\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping and permuting dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand(3,4)\n",
    "\n",
    "# Tensors can be reshaped with the view command\n",
    "# The function takes the new shape as arguments\n",
    "\n",
    "x2 = x1.view(6,2)\n",
    "x3 = x1.view(1,-1)    # The size of a dimension with -1 is computed\n",
    "x4 = x1.view(3,2,2)   # You can also add new dimensions\n",
    "\n",
    "print(x1)\n",
    "print()\n",
    "print(x2)\n",
    "print()\n",
    "print(x3)\n",
    "print()\n",
    "print(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ordering of the data is not changed.\n",
    "* The tensor is \"filled\" starting with the last dimension and ending with the first.\n",
    "* The data is not copied.\n",
    "* Hence x1, x2, x3, and x4 refers to the same underlying data.\n",
    "* So, if you change one of the tensors inplace, the other ones will also change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will try to swap (transpose) dimensions\n",
    "\n",
    "x1 = torch.rand(3,4)\n",
    "\n",
    "# The transpose function swaps the indicated dimensions\n",
    "# In this case, this corresponds to standard matrix transpose\n",
    "\n",
    "x2 = x1.transpose(0,1)\n",
    "\n",
    "# Lets compare with the effect of reshaping\n",
    "x3 = x1.view(4,3)\n",
    "\n",
    "print(x1)\n",
    "print()\n",
    "print(x2)\n",
    "print()\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note:** x2 and x3 are not the same! Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The permute function is more general and can swap multiple dimensions at the same time.\n",
    "\n",
    "x1 = torch.rand(3,4,5,6)\n",
    "\n",
    "x2 = x1.permute(2,3,0,1)   # Input the new order of dimensions\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ **Exercise**\n",
    "* Use broadcasting and the `.view(...)` function to generate a 2D multiplication table `mul_tab` of size 16x16.\n",
    "* It should contain the numbers `mul_tab[i,j] = i*j`\n",
    "* **Tip:** Check the `torch.arange` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your solution here!\n",
    "\n",
    "mul_tab = None\n",
    "\n",
    "\n",
    "print(mul_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "* Tensors can be indexed in different ways. Lets explore...\n",
    "\n",
    "#### Standard indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4,5)\n",
    "\n",
    "# Index a single value\n",
    "print(x[2,0,1])\n",
    "\n",
    "# Slice a dimension. : means 'everything'\n",
    "print(x[2,0,:])\n",
    "\n",
    "# or multiple\n",
    "print(x[:,0,:])\n",
    "\n",
    "# Slice with a range\n",
    "print(x[2, 1:, 2:-1])\n",
    "\n",
    "# Tripple dot ... means 'all remaining dimensions'\n",
    "print(x[1, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index with a list of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "# We can use LongTensor to index out a list of specific values/rows/columns\n",
    "\n",
    "ind_col = torch.LongTensor([1,1,0,3,-1,-2])   # Index these columns pls\n",
    "\n",
    "print(x[:, ind_col])\n",
    "\n",
    "\n",
    "# If we also want to index specific rows, we need to match the shapes\n",
    "\n",
    "ind_row = torch.LongTensor([2,0])   # Index these rows\n",
    "\n",
    "print(x[ind_row.view(-1,1), ind_col.view(1,-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logical indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "# Finally, we do some logical indexing\n",
    "\n",
    "logical_ind_col = torch.BoolTensor([True, False, False, True])   # Boolean tensor indicating wich columns to keep\n",
    "\n",
    "print(x[:, logical_ind_col])\n",
    "\n",
    "# The length of logical indices must match the size of the corresponding dimension (4 in this case)\n",
    "\n",
    "# Logical indexing is very useful in many cases\n",
    "# For example, say we just want to keep columns whose average is larger than 0.5:\n",
    "\n",
    "x2 = x[:, x.mean(dim=0) > 0.5]\n",
    "\n",
    "print(x2)\n",
    "\n",
    "print(x.mean(dim=0) > 0.5)  # Lets also check what the index looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is data copied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the indexed values maps to the same underlying data\n",
    "\n",
    "x = torch.rand(3,4)\n",
    "y = x[0,:]\n",
    "\n",
    "print('x =', x)\n",
    "print('y =', y)\n",
    "print()\n",
    "\n",
    "# Now, lets modify y inplace\n",
    "\n",
    "y *= 0   # This should set all values to 0\n",
    "\n",
    "print('x =', x)\n",
    "print('y =', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note:** `x` was also modified in the process!\n",
    "* Both x and y refers to the same underlying data.\n",
    "* So data is **not** copied by indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing the left-hand side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "\n",
    "# Set a specific coordinate\n",
    "x[1,1] = 999\n",
    "print(x)\n",
    "\n",
    "# Set a column to a value\n",
    "x[:,-1] = 555\n",
    "print(x)\n",
    "\n",
    "# Set a row to another tensor\n",
    "x[0,:] = -22 * torch.rand(4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining tensors\n",
    "* You can concatenate and stack tensors together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "y = 5*torch.ones(2,4)\n",
    "\n",
    "# Concatenate x and y along the first dimension (dim=0)\n",
    "z1 = torch.cat([x, y], dim=0)  # The shapes, except in dim=0, must match\n",
    "print(z1)\n",
    "\n",
    "# Stacking, on the other hand, creates a new dimension\n",
    "# Here, all dimensions must match\n",
    "z2 = torch.stack([x, x], dim=1)\n",
    "print(z2.shape)\n",
    "\n",
    "# Nicely combined with list comprehension\n",
    "z3 = torch.cat([n*torch.ones(3,n) for n in range(5)], dim=-1)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "* The key to efficiency when using PyTorch is parallelism.\n",
    "* When performing an operation, such as `z = x + y`, the computation is parallelized over all elements on the CPU or GPU.\n",
    "* It is therefore **crucial that you avoid for-loops at all costs!**\n",
    "* Complex for-loop implementations are also not compatible (or become very inefficient) with gradient computation, which we need for deep learning.\n",
    "\n",
    "### ðŸ’¡ **Exercise**\n",
    "* Here you have an example of a **very bad** implementation, which uses for-loops over the elements to compute the new tensor `z1`.\n",
    "* Implement your own solution which completely avoids loops.\n",
    "* Make sure that your result `z2` is equal to `z1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,4)\n",
    "y = torch.rand(4,7)\n",
    "u = torch.rand(3)\n",
    "v = torch.rand(7)\n",
    "\n",
    "# Example of a very bad implementation\n",
    "\n",
    "z1 = torch.zeros(x.shape[0], x.shape[1], y.shape[1])\n",
    "\n",
    "for i in range(z1.shape[0]):\n",
    "    for j in range(z1.shape[1]):\n",
    "        for k in range(z1.shape[2]):\n",
    "            diff = x[i,j] - y[j,k]\n",
    "            z1[i,j,k] = u[i] * torch.exp(diff) + v[k]\n",
    "        \n",
    "\n",
    "# Write your implementation without loops here\n",
    "\n",
    "z2 = None\n",
    "\n",
    "\n",
    "# Check that the difference is zero\n",
    "print(z1 - z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! In the next part we will use PyTorch to operate on images ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
